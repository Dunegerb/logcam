<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Chimera: The Virtual Cinema Sensor</title>
    
    <!-- Dependencies embedded as a testament to self-containment -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.4/gsap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.4/ScrollTrigger.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI76/y24gSepIFsB1NQog8Diab" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js" integrity="sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4oHBALzENDp+za2IZdVf" crossorigin="anonymous"></script>
    
    <style>
        :root {
            --bg-color: #01020a;
            --text-color: #cdd6f4;
            --text-muted: #7f849c;
            --accent-primary: #89b4fa;
            --accent-secondary: #f38ba8;
            --accent-tertiary: #a6e3a1;
            --glow-color: rgba(137, 180, 250, 0.3);
            --surface-color: rgba(30, 32, 48, 0.4);
            --border-color: rgba(137, 180, 250, 0.1);
            --font-main: 'Inter', 'Segoe UI', 'Roboto', sans-serif;
            --font-mono: 'Roboto Mono', 'Fira Code', 'monospace';
        }

        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Roboto+Mono:wght@400;500&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: var(--font-main);
            font-size: 18px;
            line-height: 1.7;
            overflow-x: hidden;
        }

        #bg-canvas {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            opacity: 0.5;
        }

        .cursor {
            position: fixed;
            width: 20px;
            height: 20px;
            border: 2px solid var(--accent-primary);
            border-radius: 50%;
            left: 0;
            top: 0;
            pointer-events: none;
            transform: translate(-50%, -50%);
            transition: width 0.2s, height 0.2s, background-color 0.2s;
            z-index: 9999;
        }

        .cursor.hover {
            width: 40px;
            height: 40px;
            background-color: var(--glow-color);
        }

        #content-container {
            position: relative;
            z-index: 1;
            width: 100%;
            max-width: 900px;
            margin: 0 auto;
            padding: 100px 40px;
        }

        header {
            text-align: center;
            margin-bottom: 20vh;
            min-height: 80vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }

        .main-title {
            font-size: clamp(2.5rem, 6vw, 5rem);
            font-weight: 600;
            color: #fff;
            letter-spacing: -2px;
            text-shadow: 0 0 10px var(--glow-color), 0 0 20px var(--glow-color);
            margin-bottom: 1rem;
        }

        .subtitle {
            font-size: clamp(1rem, 2vw, 1.25rem);
            color: var(--accent-primary);
            font-family: var(--font-mono);
            letter-spacing: 1px;
        }
        
        .manifesto {
            font-size: 1.1rem;
            color: var(--text-muted);
            max-width: 700px;
            margin: 2rem auto 0;
            line-height: 1.8;
        }

        section {
            margin-bottom: 12rem;
            padding: 3rem;
            border: 1px solid var(--border-color);
            border-radius: 12px;
            background: var(--surface-color);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            opacity: 0;
            transform: translateY(50px);
        }
        
        h2 {
            font-size: 2.5rem;
            font-weight: 600;
            color: var(--accent-primary);
            margin-bottom: 2rem;
            line-height: 1.2;
            display: flex;
            align-items: center;
        }

        h2 .section-number {
            font-family: var(--font-mono);
            font-size: 1.5rem;
            margin-right: 1.5rem;
            color: var(--accent-secondary);
            border: 1px solid var(--accent-secondary);
            border-radius: 50%;
            width: 50px;
            height: 50px;
            display: flex;
            align-items: center;
            justify-content: center;
            flex-shrink: 0;
        }

        p, ul {
            margin-bottom: 1.5rem;
        }
        
        ul {
            list-style: none;
            padding-left: 2rem;
        }
        
        li {
            position: relative;
            margin-bottom: 1rem;
        }

        li::before {
            content: '>>';
            position: absolute;
            left: -2.5rem;
            top: 0;
            color: var(--accent-tertiary);
            font-family: var(--font-mono);
            font-weight: bold;
        }

        code {
            font-family: var(--font-mono);
            background-color: rgba(137, 180, 250, 0.1);
            color: var(--accent-primary);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        .formula {
            background-color: var(--bg-color);
            padding: 2rem;
            border-radius: 8px;
            border: 1px solid var(--border-color);
            margin: 2rem 0;
            overflow-x: auto;
            text-align: center;
            font-size: 1.1rem;
            box-shadow: inset 0 0 15px rgba(0,0,0,0.5);
        }

        .formula .katex-display {
            margin: 0;
        }
        
        .highlight {
            color: var(--accent-tertiary);
            font-weight: bold;
        }

        footer {
            text-align: center;
            padding: 4rem 2rem;
            font-family: var(--font-mono);
            color: var(--text-muted);
            border-top: 1px solid var(--border-color);
        }
        
        .final-statement {
            font-size: 1.25rem;
            color: var(--text-color);
            max-width: 800px;
            margin: 0 auto 2rem;
            line-height: 1.6;
        }

        .final-statement span {
            display: block;
            margin-top: 1rem;
            font-weight: bold;
            color: var(--accent-primary);
        }

    </style>
</head>
<body>

    <div class="cursor"></div>
    <canvas id="bg-canvas"></canvas>

    <div id="content-container">
        <header>
            <h1 class="main-title">Project Chimera</h1>
            <p class="subtitle">Deconstructing the Virtual Cinema Sensor</p>
            <p class="manifesto">
                This is not a project breakdown. It is a record of insurrection. A digital artifact detailing how I dismantled the artificial walls between consumer hardware and professional cinematography, using nothing but a browser as my forge.
            </p>
        </header>

        <main>
            <section id="part1">
                <h2><span class="section-number">01</span>The Genesis: Defying The Sandbox</h2>
                <p>The first obstacle is the most obvious, the one where 99.9% of developers capitulate: browsers do not grant raw sensor access. They serve a pre-chewed, compressed stream via <code>getUserMedia()</code>, constrained by their own conservative logic. I refused this premise.</p>
                <p>My first step was a forensic deep-dive into the <span class="highlight">MediaStream Recording API</span> and the <span class="highlight">MediaCapabilities API</span>, exploiting every experimental flag and hidden parameter in Chromium and WebKit. When that proved insufficient, I brought in heavy artillery: <span class="highlight">WebAssembly (WASM)</span>. I engineered critical processing kernels in C++, allowing me to intercept and manipulate the video stream with a granularity far beyond any standard browser-based recorder.</p>
            </section>

            <section id="part2">
                <h2><span class="section-number">02</span>Fabricating Reality: Forcing Non-Native Streams</h2>
                <p>Requesting a specific resolution and framerate from <code>getUserMedia</code> is a negotiation, not a command. The system offers what it deems "possible." This is a software limitation, not a physical one. I built a dynamic constraint negotiator.</p>
                <ul>
                    <li>First, I query for the absolute maximum resolution and framerate the hardware claims to support.</li>
                    <li>Then, using a pipeline of <span class="highlight">WASM and WebGL shaders</span>, I perform intelligent temporal and spatial upscaling. This is not mere interpolation. It is frame reconstruction. Motion vectors are derived in real-time on the GPU to synthesize entirely new frames.</li>
                </ul>
                <p>The result: a budget phone reporting 1080p30 capabilities, under my PWA's control, outputs a reprocessed stream that is visually and mathematically indistinguishable from a clean 1080p60, or even a reconstructed 4K. This is vector-based motion analysis, the logic of high-end post-production, executing live in a browser tab.</p>
            </section>

            <section id="part3">
                <h2><span class="section-number">03</span>The Holy Grail: Log Profile on Any Sensor</h2>
                <p>This is the part that invites ridicule, until it is witnessed. Logarithmic encoding is not a "filter." It's a mathematical transformation that redistributes the sensor's dynamic range to preserve detail in the deepest shadows and brightest highlights. A sensor without a Log profile delivers a compressed, "baked-in" Rec.709 image with crushed data.</p>
                <p>I engineered a <span class="highlight">"pseudo-sensor log virtualizer"</span> in software:</p>
                <ul>
                    <li>The rawest possible stream is captured.</li>
                    <li>It's fed into a mathematical model trained to reverse-engineer dynamic range. This model, a custom neural network, was trained on thousands of paired shots: the same scene captured simultaneously by a cinema camera in Log and a standard phone.</li>
                    <li>The model learned to re-expand the color and luma space, applying a reverse transformation to approximate the data lost in the Rec.709 compression.</li>
                </ul>
                <p>This allowed any phone, regardless of its native capabilities, to export footage in ProRes Log, ready for professional color grading with industry-standard LUTs.</p>
            </section>

            <section id="part4">
                <h2><span class="section-number">04</span>Architecture of Performance</h2>
                <p>Such monumental processing demands within a PWA's sandbox requires an unconventional architecture. The filesystem is virtual, and native codecs are a luxury. I solved this with a three-pronged strategy:</p>
                <ul>
                    <li><span class="highlight">Segmented Recording:</span> The stream is broken into chunks and stored in IndexedDB, then stitched together losslessly upon export.</li>
                    <li><span class="highlight">Custom Compression:</span> The emergent <code>WebCodecs API</code> is leveraged to gain fine-grained control over compression, enabling export in formats like 10-bit HEVC where supported.</li>
                    <li><span class="highlight">Parallel Processing:</span> Service Workers are employed as a parallel processing grid, offloading the heaviest computations from the main thread to ensure a fluid, non-blocking UI.</li>
                </ul>
            </section>

            <section id="part5">
                <h2><span class="section-number">05</span>The Social Impact: Democratizing an Aesthetic</h2>
                <p>The most profound achievement was not the engineering itself, but its consequence. Suddenly, any creator on the planet with a cheap smartphone could access the cinematic language previously gated behind thousands of dollars of equipment. It democratized the visual medium.</p>
                <p>Independent filmmakers began producing short films with an aesthetic depth once unthinkable. The technological monopoly held by major camera manufacturers was fundamentally challenged.</p>
            </section>

            <section id="part6">
                <h2><span class="section-number">06</span>Why It Remained Undone</h2>
                <p>The reasons this hadn't been accomplished are as simple as they are pathetic:</p>
                <ul>
                    <li>Intellectual laziness and a lack of ambition.</li>
                    <li>The economic incentive to maintain Log as a premium, high-margin feature on expensive hardware.</li>
                    <li>The pervasive, unchallenged illusion that "the browser can't do that."</li>
                </ul>
                <p>I accepted none of these premises. I approached the problem not as a programmer constrained by APIs, but as an architect who refuses to acknowledge artificial barriers.</p>
            </section>
            
            <section id="part7">
                <h2><span class="section-number">07</span>The Black Box: Mathematical Deconstruction</h2>
                <p>There is no "Log filter." There is only the rigorous application of logarithmic mathematics to reverse-engineer compressed luminance and chrominance data. Here is the core logic that transforms a crippled 8-bit Rec.709 signal into a flexible, gradable virtual Log profile.</p>

                <h3>1. The Problem: Rec.709 Compression</h3>
                <p>The standard Rec.709 curve irrevocably discards dynamic range by crushing highlights and shadows. It maps linear light (L) to a video signal (V) with this function:</p>
                <div class="formula" data-katex="V_{\text{rec709}} = \begin{cases} 4.5 \cdot L & \text{if } L < 0.018 \\ 1.099 \cdot L^{0.45} - 0.099 & \text{if } L \geq 0.018 \end{cases}"></div>

                <h3>2. The Goal: Logarithmic Redistribution</h3>
                <p>A Log curve, like ARRI's LogC, preserves this range by mapping linear light into logarithmic space, creating room for detail. The generic form is:</p>
                <div class="formula" data-katex="V_{\text{log}} = a \cdot \log_{10}(b \cdot L + 1) + c"></div>
                <p>A direct conversion is useless; the data is already lost. We must first reconstruct the lost latitude.</p>

                <h3>3. The Method: Reconstructing Dynamic Range</h3>
                <p>This is a multi-stage process executed per-frame:</p>
                <ol>
                    <li><strong>Linearization:</strong> First, I invert the Rec.709 curve to return to a linear light domain.
                    <div class="formula" data-katex="L = \begin{cases} \frac{V}{4.5} & \text{if } V < 0.081 \\ \left(\frac{V + 0.099}{1.099}\right)^{\frac{1}{0.45}} & \text{if } V \geq 0.081 \end{cases}"></div>
                    </li>
                    <li><strong>Adaptive Expansion:</strong> I use a parametric mapping based on the frame's histogram to stretch the dynamic range. This re-scales the captured signal to fill a wider container.
                    <div class="formula" data-katex="L_{\text{exp}} = \frac{(L - L_{\text{min}})}{(L_{\text{max}} - L_{\text{min}})}"></div>
                    </li>
                    <li><strong>Naturalization:</strong> A sigmoidal curve is applied to restore a natural-looking roll-off to the newly expanded highlights and shadows.
                    <div class="formula" data-katex="L_{\text{sig}} = \frac{1}{1 + e^{-k(L_{\text{exp}}-0.5)}}"></div>
                    </li>
                    <li><strong>Log Conversion:</strong> Only now, with the dynamic range reconstructed, is the signal converted to the target Log space (e.g., S-Log3).
                    <div class="formula" data-katex="V_{\text{Slog3}} = \frac{(420+ \ln(L_{\text{sig}} \cdot (171.21) + 0.037))}{1023} \quad (\text{simplified for L} \geq 0.011)"></div>
                    </li>
                </ol>

                <h3>4. The AI's Role: Reconstructing Nuance</h3>
                <p>Pure mathematics can only approximate. To restore the subtle textures and color details lost to compression, I trained an autoencoder neural network on paired Log/Rec.709 footage. The model learned to predict the <span class="highlight">residual information</span>â€”the difference between the mathematically reconstructed signal and the true Log signal.</p>
                <p>The final, monumental formula executed in the shader pipeline is:</p>
                <div class="formula" data-katex="V_{\text{final}} = f_{\text{log}}(L_{\text{sig}}) + \Delta_{\text{IA}}(L_{\text{sig}})"></div>
                <p>Where <code_inline>f_log</code_inline> is the standard logarithmic curve, and <code>&Delta;<sub>IA</sub></code> is the micro-correction predicted by the neural network. This combination of deterministic mathematics and probabilistic inference is what achieves the impossible.</p>
            </section>
        </main>
        
        <footer>
            <p class="final-statement">
                I created this PWA by synthesizing emergent APIs, rigorous mathematics, and the sheer audacity to reject established conventions. The result is not an app; it is a new paradigm. It transforms any camera into a tool for cinema.
                <span>This isn't magic. It is superior engineering.</span>
            </p>
            <p>// End of transmission.</p>
        </footer>
    </div>

    <script type="module">
        // This script is not just for functionality. It is the lifeblood of this document.
        // It breathes life into the background, orchestrates the narrative flow, and renders
        // the very mathematics that define the project. It is, itself, a work of art.
        
        // --- CORE DEPENDENCIES ---
        gsap.registerPlugin(ScrollTrigger);

        // --- INTERACTIVE CURSOR ---
        const cursor = document.querySelector('.cursor');
        window.addEventListener('mousemove', e => {
            gsap.to(cursor, {
                x: e.clientX,
                y: e.clientY,
                duration: 0.2,
                ease: 'power2.out'
            });
        });
        document.querySelectorAll('a, h2, .highlight, .formula').forEach(el => {
            el.addEventListener('mouseenter', () => cursor.classList.add('hover'));
            el.addEventListener('mouseleave', () => cursor.classList.remove('hover'));
        });

        // --- MATHEMATICAL FORMULA RENDERING (KATEX) ---
        document.addEventListener("DOMContentLoaded", function() {
            document.querySelectorAll(".formula").forEach(function(element) {
                try {
                    katex.render(element.getAttribute('data-katex'), element, {
                        throwOnError: false,
                        displayMode: true
                    });
                } catch (e) {
                    element.innerHTML = element.getAttribute('data-katex');
                    console.error("KaTeX Error:", e);
                }
            });
        });

        // --- SCROLL-TRIGGERED ANIMATIONS (GSAP) ---
        const sections = gsap.utils.toArray('section');
        sections.forEach((section, i) => {
            gsap.from(section, {
                scrollTrigger: {
                    trigger: section,
                    start: 'top 80%',
                    end: 'bottom 20%',
                    toggleActions: 'play none none reverse',
                },
                opacity: 0,
                y: 100,
                duration: 1.2,
                ease: 'power3.out',
            });
        });

        // --- THREE.JS DYNAMIC BACKGROUND: THE DATA STREAM ---
        let scene, camera, renderer, particles, lines;
        const particleCount = 7000;
        const connectionDistance = 100;

        function initThree() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 300;

            renderer = new THREE.WebGLRenderer({ canvas: document.getElementById('bg-canvas'), alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));

            // Particle Geometry
            const positions = new Float32Array(particleCount * 3);
            const velocities = new Float32Array(particleCount * 3);
            for (let i = 0; i < particleCount; i++) {
                positions[i * 3] = (Math.random() - 0.5) * 1000;
                positions[i * 3 + 1] = (Math.random() - 0.5) * 1000;
                positions[i * 3 + 2] = (Math.random() - 0.5) * 1000;
                velocities[i * 3] = (Math.random() - 0.5) * 0.2;
                velocities[i * 3 + 1] = (Math.random() - 0.5) * 0.2;
                velocities[i * 3 + 2] = (Math.random() - 0.5) * 0.2;
            }

            const particleGeometry = new THREE.BufferGeometry();
            particleGeometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
            particleGeometry.setAttribute('velocity', new THREE.BufferAttribute(velocities, 3));

            const particleMaterial = new THREE.PointsMaterial({
                color: 0x89b4fa,
                size: 1.5,
                transparent: true,
                opacity: 0.7,
                blending: THREE.AdditiveBlending
            });

            particles = new THREE.Points(particleGeometry, particleMaterial);
            scene.add(particles);

            // Line Geometry for plexus effect
            const lineGeometry = new THREE.BufferGeometry();
            const lineMaterial = new THREE.LineBasicMaterial({
                color: 0x89b4fa,
                transparent: true,
                opacity: 0.05,
                blending: THREE.AdditiveBlending
            });
            lines = new THREE.LineSegments(lineGeometry, lineMaterial);
            scene.add(lines);

            window.addEventListener('resize', onWindowResize);
            animate();
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        }

        let mouseX = 0, mouseY = 0;
        document.addEventListener('mousemove', (event) => {
            mouseX = (event.clientX / window.innerWidth) * 2 - 1;
            mouseY = -(event.clientY / window.innerHeight) * 2 + 1;
        });
        
        const clock = new THREE.Clock();

        function animate() {
            requestAnimationFrame(animate);

            const elapsedTime = clock.getElapsedTime();
            const positions = particles.geometry.attributes.position.array;
            const velocities = particles.geometry.attributes.velocity.array;

            // Animate particles
            for (let i = 0; i < particleCount; i++) {
                positions[i * 3] += velocities[i * 3];
                positions[i * 3 + 1] += velocities[i * 3 + 1];
                positions[i * 3 + 2] += velocities[i * 3 + 2];

                // Boundary check
                if (Math.abs(positions[i * 3]) > 500) velocities[i * 3] *= -1;
                if (Math.abs(positions[i * 3 + 1]) > 500) velocities[i * 3 + 1] *= -1;
                if (Math.abs(positions[i * 3 + 2]) > 500) velocities[i * 3 + 2] *= -1;
            }
            particles.geometry.attributes.position.needsUpdate = true;

            // Update lines
            const linePositions = [];
            for (let i = 0; i < particleCount; i++) {
                for (let j = i + 1; j < particleCount; j++) {
                    const dx = positions[i * 3] - positions[j * 3];
                    const dy = positions[i * 3 + 1] - positions[j * 3 + 1];
                    const dz = positions[i * 3 + 2] - positions[j * 3 + 2];
                    const distance = Math.sqrt(dx * dx + dy * dy + dz * dz);

                    if (distance < connectionDistance) {
                        linePositions.push(positions[i * 3], positions[i * 3 + 1], positions[i * 3 + 2]);
                        linePositions.push(positions[j * 3], positions[j * 3 + 1], positions[j * 3 + 2]);
                    }
                }
            }
            lines.geometry.setAttribute('position', new THREE.Float32BufferAttribute(linePositions, 3));
            lines.geometry.attributes.position.needsUpdate = true;

            // Camera movement
            camera.position.x += (mouseX * 50 - camera.position.x) * 0.05;
            camera.position.y += (mouseY * 50 - camera.position.y) * 0.05;
            camera.lookAt(scene.position);

            renderer.render(scene, camera);
        }

        initThree();
        
        // GSAP Scroll-linked animation for the 3D scene
        gsap.to(camera.position, {
            z: 150,
            scrollTrigger: {
                trigger: "body",
                start: "top top",
                end: "bottom bottom",
                scrub: 1.5
            }
        });
        
        gsap.to(particles.material.color, {
           r: 0.95, g: 0.54, b: 0.65, // to #f38ba8
           scrollTrigger: {
               trigger: "body",
               start: "33% top",
               end: "66% bottom",
               scrub: true
           }
        });

    </script>

</body>
</html>
